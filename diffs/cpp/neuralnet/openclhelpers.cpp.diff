--- cpp/neuralnet/openclhelpers.cpp [lightvector:master]+++ cpp/neuralnet/openclhelpers.cpp [hzyhhzy:Reversi2022]@@ -100,7 +100,7 @@ 
   const string opts = options + " -cl-mad-enable -cl-fast-relaxed-math -cl-no-signed-zeros -cl-denorms-are-zero";
 
-  err = clBuildProgram(program, (cl_uint)devices.size(), devices.data(), opts.c_str(), NULL, NULL);
+  err = clBuildProgram(program, devices.size(), devices.data(), opts.c_str(), NULL, NULL);
   if(err != 0) {
     string s;
     s += OpenCLHelpers::getErrorMessage(err) + string("\n");
@@ -259,7 +259,7 @@   cl_int err;
   cl_uint numPlatforms;
   vector<cl_platform_id> platformIds(maxPlatforms);
-  err = clGetPlatformIDs((cl_uint)platformIds.size(), platformIds.data(), &numPlatforms);
+  err = clGetPlatformIDs(platformIds.size(), platformIds.data(), &numPlatforms);
   CHECK_ERR(err);
   assert(numPlatforms <= platformIds.size());
   platformIds.resize(numPlatforms);
@@ -273,7 +273,7 @@   vector<cl_device_id> deviceIds(maxDevices);
   vector<cl_platform_id> platformIdsForDevices;
   vector<string> platformDescsForDevices;
-  for(cl_uint platformIdx = 0; platformIdx < numPlatforms && numDevicesTotal < deviceIds.size(); platformIdx++) {
+  for(int platformIdx = 0; platformIdx < numPlatforms && numDevicesTotal < deviceIds.size(); platformIdx++) {
     size_t sizeRet;
     cl_platform_id platformId = platformIds[platformIdx];
 
@@ -294,20 +294,20 @@ 
     string desc =  name + " (" + vendor + ") (" + version + ")";
     if(logger != NULL)
-      logger->write("Found OpenCL Platform " + Global::uint32ToString(platformIdx) + ": " + desc);
+      logger->write("Found OpenCL Platform " + Global::intToString(platformIdx) + ": " + desc);
 
     cl_uint numDevices;
     err = clGetDeviceIDs(
-      platformId, CL_DEVICE_TYPE_CPU | CL_DEVICE_TYPE_GPU | CL_DEVICE_TYPE_ACCELERATOR, (cl_uint)(deviceIds.size() - numDevicesTotal),
+      platformId, CL_DEVICE_TYPE_CPU | CL_DEVICE_TYPE_GPU | CL_DEVICE_TYPE_ACCELERATOR, deviceIds.size() - numDevicesTotal,
       deviceIds.data() + numDevicesTotal, &numDevices);
     //Allow there to be 0 devices on this platform, just move on to the next
     if(err == CL_DEVICE_NOT_FOUND) {
       if(logger != NULL)
-        logger->write("Found 0 device(s) on platform " + Global::uint32ToString(platformIdx) + " with type CPU or GPU or Accelerator, skipping");
+        logger->write("Found 0 device(s) on platform " + Global::intToString(platformIdx) + " with type CPU or GPU or Accelerator, skipping");
       continue;
     }
 
-    for(cl_uint i = 0; i < numDevices; i++) {
+    for(size_t i = 0; i < numDevices; i++) {
       platformIdsForDevices.push_back(platformId);
       platformDescsForDevices.push_back(desc);
     }
@@ -316,7 +316,7 @@     numDevicesTotal += numDevices;
     assert(numDevicesTotal <= deviceIds.size());
     if(logger != NULL)
-      logger->write("Found " + Global::uint32ToString(numDevices) + " device(s) on platform " + Global::uint32ToString(platformIdx) + " with type CPU or GPU or Accelerator");
+      logger->write("Found " + Global::intToString(numDevices) + " device(s) on platform " + Global::intToString(platformIdx) + " with type CPU or GPU or Accelerator");
   }
   deviceIds.resize(numDevicesTotal);
 
@@ -493,7 +493,7 @@     cl_int err;
     initializedPlatform->context = clCreateContext(
       initializedPlatform->properties.data(),
-      (cl_uint)initializedPlatform->deviceIdsToUseForThisPlatform.size(),
+      initializedPlatform->deviceIdsToUseForThisPlatform.size(),
       initializedPlatform->deviceIdsToUseForThisPlatform.data(),
       NULL,
       NULL,
@@ -609,12 +609,6 @@ 
 size_t OpenCLHelpers::roundUpToMultiple(size_t size, size_t ofThis) {
   return (size + ofThis - 1) / ofThis * ofThis;
-}
-
-int OpenCLHelpers::roundUpToMultipleInt(size_t size, size_t ofThis) {
-  size_t result = (size + ofThis - 1) / ofThis * ofThis;
-  assert(result <= (size_t)0x7FFFffffULL);
-  return (int)result;
 }
 
 cl_int OpenCLHelpers::doBatchedXGemm_KM_KN_NM(
@@ -833,8 +827,8 @@   int convSize,
   cl_event* eventBuf
 ) {
-  int inChannelsPadded = roundUpToMultipleInt(inChannels, inChannelsPadMultiple);
-  int batchNumTilesPadded = roundUpToMultipleInt(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
+  int inChannelsPadded = roundUpToMultiple(inChannels, inChannelsPadMultiple);
+  int batchNumTilesPadded = roundUpToMultiple(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
 
   clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&input);
   clSetKernelArg(kernel, 1, sizeof(cl_mem), (void *)&convWorkspace);
@@ -877,8 +871,8 @@   int convSize,
   cl_event* eventBuf
 ) {
-  int inChannelsPadded = roundUpToMultipleInt(inChannels, inChannelsPadMultiple);
-  int batchNumTilesPadded = roundUpToMultipleInt(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
+  int inChannelsPadded = roundUpToMultiple(inChannels, inChannelsPadMultiple);
+  int batchNumTilesPadded = roundUpToMultiple(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
 
   clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&input);
   clSetKernelArg(kernel, 1, sizeof(cl_mem), (void *)&convWorkspace);
@@ -923,8 +917,8 @@   int convSize,
   cl_event* eventBuf
 ) {
-  int outChannelsPadded = roundUpToMultipleInt(outChannels, outChannelsPadMultiple);
-  int batchNumTilesPadded = roundUpToMultipleInt(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
+  int outChannelsPadded = roundUpToMultiple(outChannels, outChannelsPadMultiple);
+  int batchNumTilesPadded = roundUpToMultiple(batchSize * numTilesX * numTilesY, batchNumTilesPadMultiple);
 
   clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&convWorkspace2);
   clSetKernelArg(kernel, 1, sizeof(cl_mem), (void *)&output);
